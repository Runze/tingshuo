{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import itertools\n",
    "import spacy\n",
    "from collections import Counter, defaultdict\n",
    "import io\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from IPython.display import IFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('utils')\n",
    "from preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.options.display.max_rows = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_data = pickle.load(open('data/podcast_data_translated.pkl', 'rb'))\n",
    "podcasts, podcast_id_to_episodes = podcast_data['podcasts'], podcast_data['podcast_id_to_episodes']\n",
    "podcasts.shape, len(podcast_id_to_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tokens\n",
    "def extract_tokens(sent, ents_to_rm=None, rm_stop=False, rm_punct=False):\n",
    "    if ents_to_rm:\n",
    "        sent = [token for token in sent if token.ent_type_ not in ents_to_rm]\n",
    "    if rm_stop:\n",
    "        sent = [token for token in sent if not token.is_stop]\n",
    "    if rm_punct:\n",
    "        sent = [token for token in sent if not token.is_punct]\n",
    "    return [token.text.lower() for token in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_tokens = podcasts['summary_episodes_en_cleaned'].apply(lambda sents: [extract_tokens(sent, ents_to_rm=['GPE', 'NORP', 'PERSON'], rm_stop=True, rm_punct=True) for sent in sents]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate tokens from the same podcast\n",
    "podcast_tokens = [list(itertools.chain(*podcast)) for podcast in podcast_tokens]\n",
    "len(podcast_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab_mapper(toks, max_vocab=100000, min_freq=2, UNK='_unk_'):\n",
    "    toks_freq = Counter(toks)    \n",
    "    itos = [s for s, c in toks_freq.most_common(max_vocab) if c >= min_freq]\n",
    "    \n",
    "    if UNK:\n",
    "        itos.insert(0, UNK)  # Note the index for UNK is 0\n",
    "        stoi = defaultdict(lambda: 0, {v: k for k, v in enumerate(itos)})\n",
    "    else:\n",
    "        stoi = {v: k for k, v in enumerate(itos)}\n",
    "    \n",
    "    return stoi, itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all the tokens\n",
    "podcast_tokens_combined = list(itertools.chain(*podcast_tokens))\n",
    "len(podcast_tokens_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi, itos = create_vocab_mapper(podcast_tokens_combined)\n",
    "len(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map to pretrained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process fasttext embeddings\n",
    "# https://fasttext.cc/docs/en/english-vectors.html\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = np.array(list(map(float, tokens[1:])))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_word_embed = load_vectors('data/wiki-news-300d-1M-subword.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each word to the pre-trained word vectors\n",
    "itoe = np.zeros((len(itos), 300))\n",
    "not_found = []\n",
    "\n",
    "for i, token in enumerate(itos):\n",
    "    if token in pretrained_word_embed:\n",
    "        itoe[i] = np.array(pretrained_word_embed[token])\n",
    "    else:\n",
    "        not_found.append(token)\n",
    "\n",
    "print('% of tokens not found in the pretrained embeddings: {}'.format(len(not_found) / len(itoe)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_found[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "itoe = normalize(itoe)\n",
    "itoe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute podcast embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_podcast_embeddings(podcast, stoi, itoe):\n",
    "    # Map to indices\n",
    "    podcast_ix = [stoi[token] for token in podcast]\n",
    "\n",
    "    # Map to word embeddings\n",
    "    podcast_embeddings = [itoe[ix] for ix in podcast_ix]\n",
    "    \n",
    "    # Compute average over all word embeddings\n",
    "    return np.array(podcast_embeddings).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_embeddings = [compute_podcast_embeddings(podcast, stoi, itoe) for podcast in podcast_tokens]\n",
    "podcast_embeddings = np.array(podcast_embeddings)\n",
    "podcast_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "podcast_embeddings = normalize(podcast_embeddings)\n",
    "podcast_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_embed_to_2d(embeddings, algorithm='tsne'):\n",
    "    if embeddings.shape[1] > 2:\n",
    "        if algorithm == 'pca':\n",
    "            embeddings = PCA(n_components=2, random_state=0).fit_transform(embeddings)\n",
    "        elif algorithm == 'tsne':\n",
    "            embeddings = TSNE(n_components=2, init='pca', random_state=0).fit_transform(embeddings)\n",
    "    \n",
    "    # Convert to dataframe\n",
    "    embeddings = pd.DataFrame(embeddings, columns=['x', 'y'])\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_embeddings_2d = reduce_embed_to_2d(podcast_embeddings)\n",
    "podcast_embeddings_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add podcast attributes\n",
    "podcast_embeddings_2d[['country_fullname', 'podcast_id', 'im_name_label', 'summary_label', 'summary_label_en_cleaned', 'link_attributes_href', 'feedurl', 'artwork']] = podcasts[['country_fullname', 'podcast_id', 'im_name_label', 'summary_label', 'summary_label_en_cleaned', 'link_attributes_href', 'feedurl', 'artwork']].copy()\n",
    "\n",
    "# Only keep the first few sentences of each description\n",
    "def cut_sents(sents, max_len=150):\n",
    "    if len(sents) <= 1:\n",
    "        return sents[0] + ' ...'\n",
    "    cutoff = np.where(np.cumsum([len(sent) for sent in sents]) >= max_len)[0]\n",
    "    if len(cutoff) > 0:\n",
    "        sents = sents[:(cutoff[0]+1)]\n",
    "    return ' '.join(sents) + ' ...'\n",
    "\n",
    "# Add hover text\n",
    "podcast_embeddings_2d['summary_label_en_cleaned_brief'] = [cut_sents(sents) for sents in podcast_embeddings_2d['summary_label_en_cleaned'].tolist()]\n",
    "podcast_embeddings_2d['summary_label_en_cleaned_brief'] = podcast_embeddings_2d['summary_label_en_cleaned_brief'].str.wrap(50).str.replace('\\n', '<br>')\n",
    "podcast_embeddings_2d['hover_text'] = podcast_embeddings_2d['im_name_label'] + '<br>' + podcast_embeddings_2d['summary_label_en_cleaned_brief']\n",
    "\n",
    "# Clean up summaries\n",
    "podcast_embeddings_2d['summary_label'] = podcast_embeddings_2d['summary_label'].str.join(' ')\n",
    "podcast_embeddings_2d['summary_label_en_cleaned'] = podcast_embeddings_2d['summary_label_en_cleaned'].str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the same podcast has the same (x, y)\n",
    "podcast_embeddings_2d_unique = podcast_embeddings_2d.groupby('podcast_id')[['x', 'y']].mean().reset_index()\n",
    "podcast_embeddings_2d = pd.merge(podcast_embeddings_2d.drop(['x', 'y'], axis=1).reset_index(), podcast_embeddings_2d_unique)\n",
    "podcast_embeddings_2d.sort_values('index', inplace=True)\n",
    "podcast_embeddings_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove podcasts that are not in the country list\n",
    "podcast_embeddings_2d = podcast_embeddings_2d[podcast_embeddings_2d['country_fullname'].isin(countries.values())].copy()\n",
    "podcast_embeddings_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep the top N podcasts per country\n",
    "n_podcasts = 30\n",
    "podcast_embeddings_2d = podcast_embeddings_2d.groupby('country_fullname').head(n_podcasts)\n",
    "podcast_embeddings_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save plot data\n",
    "podcast_embeddings_2d.to_csv('data/podcast_embeddings_2d.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract podcast descriptions\n",
    "podcast_id_to_desc = podcast_embeddings_2d[['podcast_id', 'im_name_label', 'summary_label', 'summary_label_en_cleaned', 'link_attributes_href', 'feedurl', 'artwork']].copy()\n",
    "podcast_id_to_desc['is_us'] = podcast_embeddings_2d['country_fullname'] == 'United States'\n",
    "podcast_id_to_desc.sort_values(['podcast_id', 'is_us'], ascending=[True, False], inplace=True)\n",
    "podcast_id_to_desc = podcast_id_to_desc.drop('is_us', axis=1).drop_duplicates(subset='podcast_id', keep='first')\n",
    "podcast_id_to_desc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dict\n",
    "podcast_id_to_desc = podcast_id_to_desc.set_index('podcast_id').to_dict('index')\n",
    "for podcast in podcast_id_to_desc.values():\n",
    "    if podcast['summary_label'] == podcast['summary_label_en_cleaned']:\n",
    "        podcast.pop('summary_label_en_cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "pickle.dump(podcast_id_to_desc, open('data/podcast_id_to_desc.pkl', 'wb'), protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up episodes data for display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_summary_w_en(episode):\n",
    "    pairs = pd.DataFrame(zip(episode['summary'], episode['summary_en_cleaned']), columns=['original', 'translated']).to_dict('records')\n",
    "    for pair in pairs:\n",
    "        if pair['original'] == pair['translated']:\n",
    "            pair.pop('translated')\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_id_to_episodes_cleaned = {}\n",
    "n_episodes = 30\n",
    "for podcast_id in podcast_embeddings_2d['podcast_id'].unique():\n",
    "    episodes = podcast_id_to_episodes[podcast_id].copy()\n",
    "    \n",
    "    # Keep only the recent episodes\n",
    "    episodes = episodes.head(n_episodes)\n",
    "    \n",
    "    # Clean up\n",
    "    episodes = episodes[['date', 'title', 'title_en_cleaned', 'summary', 'summary_en_cleaned', 'link']]\n",
    "    episodes['date'] = pd.to_datetime(episodes['date'], utc=True).dt.date.astype('str')\n",
    "    episodes['title'] = episodes['title'].str.join(' ')\n",
    "    episodes['title_en_cleaned'] = episodes['title_en_cleaned'].str.join(' ')\n",
    "    \n",
    "    # Convert to dict\n",
    "    episodes = episodes.to_dict('records')\n",
    "\n",
    "    # Pair summary and its translations\n",
    "    for episode in episodes:\n",
    "        if episode['title'] == episode['title_en_cleaned']:\n",
    "            episode.pop('title_en_cleaned')\n",
    "        \n",
    "        episode['summary_w_en'] = pair_summary_w_en(episode)\n",
    "        episode.pop('summary')\n",
    "        episode.pop('summary_en_cleaned')\n",
    "    \n",
    "    podcast_id_to_episodes_cleaned[podcast_id] = episodes\n",
    "\n",
    "len(podcast_id_to_episodes_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "pickle.dump(podcast_id_to_episodes_cleaned, open('data/podcast_id_to_episodes_cleaned.pkl', 'wb'), protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (General DS)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
