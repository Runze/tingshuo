{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from polyglot.detect import Detector\n",
    "from polyglot.detect.base import UnknownLanguage\n",
    "from google.cloud import translate\n",
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'cred/translate-cred.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('utils')\n",
    "from preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.options.display.max_rows = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_data = pickle.load(open('data/podcast_data_proc.pkl', 'rb'))\n",
    "podcasts, podcast_id_to_episodes = podcast_data['podcasts'], podcast_data['podcast_id_to_episodes']\n",
    "podcasts.shape, len(podcast_id_to_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full-join with the previously-translated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previously-translated data\n",
    "podcast_data_translated = pickle.load(open('data/podcast_data_translated.pkl', 'rb'))\n",
    "podcasts_translated, podcast_id_to_episodes_translated = podcast_data_translated['podcasts'], podcast_data_translated['podcast_id_to_episodes']\n",
    "podcasts_translated.shape, len(podcast_id_to_episodes_translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_two_dfs(df_1, df_2, id_col, sort_cols, ascending=True):\n",
    "    df_1 = df_1.copy()\n",
    "    df_2 = df_2.copy()\n",
    "    \n",
    "    # Make sure that `df_2` includes all the columns in `df_1`\n",
    "    assert np.array_equal(sorted(df_1.columns), sorted(np.intersect1d(df_1.columns, df_2.columns)))\n",
    "\n",
    "    # Create placeholder columns for `df_1`\n",
    "    for col in np.setdiff1d(df_2.columns, df_1.columns):\n",
    "        df_1[col] = np.nan\n",
    "\n",
    "    # De-dupe `df_2`\n",
    "    df_2.drop_duplicates(id_col, inplace=True)\n",
    "\n",
    "    # Index using the ID column\n",
    "    df_1.reset_index(inplace=True)\n",
    "    df_2.reset_index(inplace=True)\n",
    "    df_1 = df_1.set_index(id_col)\n",
    "    df_2 = df_2.set_index(id_col)\n",
    "\n",
    "    # Combine\n",
    "    df_1['source'] = 0\n",
    "    df_2['source'] = 1\n",
    "    df = df_1.combine_first(df_2)\n",
    "\n",
    "    # Clean up\n",
    "    df.sort_values(sort_cols, ascending=ascending, inplace=True)\n",
    "    df = df[df_1.columns]\n",
    "    df.drop(['index', 'source'], axis=1, inplace=True)\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine podcasts\n",
    "podcasts = combine_two_dfs(podcasts, podcasts_translated, id_col='podcast_id', sort_cols=['country', 'source', 'index'])\n",
    "podcasts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine episodes\n",
    "for podcast_id in podcasts['podcast_id'].unique():\n",
    "    if podcast_id in podcast_id_to_episodes_translated:\n",
    "        if podcast_id in podcast_id_to_episodes:\n",
    "            podcast_id_to_episodes[podcast_id] = combine_two_dfs(podcast_id_to_episodes[podcast_id], podcast_id_to_episodes_translated[podcast_id], id_col='episode_id', sort_cols=['date'], ascending=False)\n",
    "        else:\n",
    "            # If a podcast is not found among the top anymore, do not update or translate its episodes\n",
    "            podcast_id_to_episodes[podcast_id] = podcast_id_to_episodes_translated[podcast_id].copy()\n",
    "    else:\n",
    "        for col in ['title_en', 'title_en_cleaned', 'summary_en', 'summary_en_cleaned', 'summary_en_cleaned_deduped']:\n",
    "            podcast_id_to_episodes[podcast_id][col] = np.nan\n",
    "\n",
    "len(podcast_id_to_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_client = translate.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_using_google(text):\n",
    "    translation = translate_client.translate(text, target_language='en', model='nmt')\n",
    "    return translation['detectedSourceLanguage'], translation['translatedText']\n",
    "\n",
    "def translate(text, detect_lang=False):\n",
    "    try:\n",
    "        # First detect the language offline\n",
    "        detected = Detector(text)\n",
    "        lang, reliable = detected.language.code, detected.reliable\n",
    "        \n",
    "        # Only translate if not in English\n",
    "        if lang != 'en' or not reliable:\n",
    "            translated = translate_using_google(text)\n",
    "        else:\n",
    "            translated = ('en', text)\n",
    "\n",
    "    except UnknownLanguage:\n",
    "        translated = translate_using_google(text)\n",
    "\n",
    "    except Exception:\n",
    "        translated = ('en', text)\n",
    "\n",
    "    if detect_lang:\n",
    "        return translated\n",
    "    return translated[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sents(sents, detect_lang=False):\n",
    "    if sents:\n",
    "        translated = [translate(sent, detect_lang) for sent in sents]\n",
    "        if detect_lang:\n",
    "            # Seperate the detected language from the translation\n",
    "            src_lang, translated_sents = zip(*translated)\n",
    "\n",
    "            # Find the primary language\n",
    "            src_lang = pd.Series(src_lang).value_counts().index[0]\n",
    "\n",
    "            # Return the tuple\n",
    "            return src_lang, list(translated_sents)\n",
    "        return translated\n",
    "    else:\n",
    "        if detect_lang:\n",
    "            return np.nan, []\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_df(df, cols_to_translate, detect_langs):\n",
    "    df = df.copy()\n",
    "    for col, detect_lang in zip(cols_to_translate, detect_langs):\n",
    "        # Only translate the records that have not been translated\n",
    "        if col+'_en' not in df:\n",
    "            df[col+'_en'] = np.nan\n",
    "        if detect_lang:\n",
    "            if 'src_lang' not in df:\n",
    "                df['src_lang'] = np.nan\n",
    "            translated = df.loc[df[col+'_en'].isnull(), col].apply(lambda x: translate_sents(x, True)).tolist()\n",
    "            if translated:\n",
    "                df.loc[df[col+'_en'].isnull(), ['src_lang', col+'_en']] = translated\n",
    "        else:\n",
    "            df.loc[df[col+'_en'].isnull(), col+'_en'] = df.loc[df[col+'_en'].isnull(), col].apply(translate_sents)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate podcast summaries\n",
    "podcasts = translate_df(podcasts, cols_to_translate=['summary_label'], detect_langs=[False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate episode titles and summaries\n",
    "for podcast_id in podcast_id_to_episodes:\n",
    "    podcast_id_to_episodes[podcast_id] = translate_df(podcast_id_to_episodes[podcast_id], cols_to_translate=['title', 'summary'], detect_langs=[False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "podcast_data = {\n",
    "    'podcasts': podcasts,\n",
    "    'podcast_id_to_episodes': podcast_id_to_episodes\n",
    "}\n",
    "\n",
    "pickle.dump(podcast_data, open('data/podcast_data_proc.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sents(sents):\n",
    "    return [clean_text(sent) for sent in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean podcast summaries\n",
    "podcasts['summary_label_en_cleaned'] = podcasts['summary_label_en'].apply(clean_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean episode titles and summaries\n",
    "for podcast_id in podcast_id_to_episodes:\n",
    "    podcast_id_to_episodes[podcast_id]['title_en_cleaned'] = podcast_id_to_episodes[podcast_id]['title_en'].apply(clean_sents)\n",
    "    podcast_id_to_episodes[podcast_id]['summary_en_cleaned'] = podcast_id_to_episodes[podcast_id]['summary_en'].apply(clean_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate summaries among episodes\n",
    "for episodes in podcast_id_to_episodes.values():\n",
    "    episodes['summary_en_cleaned_deduped'] = remove_duplicate_summaries(episodes['summary_en_cleaned'].tolist(), episodes['title_en_cleaned'].tolist(), dedupe_within_summaries=True, need_sent_tokenization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp data/podcast_data_translated.pkl data/podcast_data_translated-OLD.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "podcast_data = {\n",
    "    'podcasts': podcasts,\n",
    "    'podcast_id_to_episodes': podcast_id_to_episodes\n",
    "}\n",
    "\n",
    "pickle.dump(podcast_data, open('data/podcast_data_translated.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (General DS)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
