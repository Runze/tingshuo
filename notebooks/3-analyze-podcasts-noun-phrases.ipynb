{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import itertools\n",
    "import spacy\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import colorlover as cl\n",
    "from IPython.display import IFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('utils')\n",
    "from preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.options.display.max_rows = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load translated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_data = pickle.load(open('data/podcast_data_translated.pkl', 'rb'))\n",
    "podcasts, podcast_id_to_episodes = podcast_data['podcasts'], podcast_data['podcast_id_to_episodes']\n",
    "podcasts.shape, len(podcast_id_to_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcasts['country_fullname'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine podcast descriptions and episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, combine episode titles and summaries\n",
    "podcast_id_to_episodes_combined = {}\n",
    "for podcast_id, episodes in podcast_id_to_episodes.items():\n",
    "    # Combine titles and summaries for each episode\n",
    "    title_summary_en_cleaned = episodes['title_en_cleaned'] + episodes['summary_en_cleaned_deduped']\n",
    "    \n",
    "    # Concatenate all episodes\n",
    "    podcast_id_to_episodes_combined[podcast_id] = list(itertools.chain(*title_summary_en_cleaned.tolist()))\n",
    "\n",
    "len(podcast_id_to_episodes_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, combine episodes with podcast summaries\n",
    "episodes_en_cleaned = podcasts['podcast_id'].map(podcast_id_to_episodes_combined)\n",
    "summary_episodes_en_cleaned = podcasts['summary_label_en_cleaned'] + episodes_en_cleaned\n",
    "len(summary_episodes_en_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize combined podcast summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_list_of_sents_w_spacy(sents):\n",
    "    return [tokens for tokens in spacy_en.pipe(sents, n_threads=10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize combined episodes with podcast summaries\n",
    "podcasts['summary_episodes_en_cleaned'] = summary_episodes_en_cleaned.apply(tokenize_list_of_sents_w_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "podcast_data = {\n",
    "    'podcasts': podcasts,\n",
    "    'podcast_id_to_episodes': podcast_id_to_episodes\n",
    "}\n",
    "\n",
    "pickle.dump(podcast_data, open('data/podcast_data_translated.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare noun phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_data = pickle.load(open('data/podcast_data_translated.pkl', 'rb'))\n",
    "podcasts, podcast_id_to_episodes = podcast_data['podcasts'], podcast_data['podcast_id_to_episodes']\n",
    "podcasts.shape, len(podcast_id_to_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove podcasts that are not in the country list\n",
    "podcasts = podcasts[podcasts['country'].isin(countries)].copy()\n",
    "podcasts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcasts['country_fullname'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract noun phrases\n",
    "def extract_noun_phrases(sent):\n",
    "    phrases = [token for token in sent.noun_chunks]\n",
    "    \n",
    "    # Only keep nouns (specifically their lemmas)\n",
    "    phrases = [[token.lemma_.lower() for token in phrase if token.pos_ in ['NOUN']] for phrase in phrases]\n",
    "    \n",
    "    # Clean up\n",
    "    return [' '.join(phrase) for phrase in phrases if phrase]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_episodes_en_cleaned_phrases = podcasts['summary_episodes_en_cleaned'].apply(lambda sents: list(itertools.chain(*[extract_noun_phrases(sent) for sent in sents])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count noun phrases\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=lambda text: text, lowercase=False, min_df=5, stop_words=\"english\")\n",
    "phrase_count = tfidf_vectorizer.fit_transform(summary_episodes_en_cleaned_phrases)\n",
    "phrase_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "phrase_count = phrase_count.toarray()\n",
    "phrase_count = pd.DataFrame(phrase_count, columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "patterns_to_rm = ['issue', 'episode', 'podcast', 'program', 'trailer', 'floor', 'section', 'addition', 'guest', 'listener', 'host', 'first order', 'tune', 'editor', 'discount']\n",
    "phrase_count = phrase_count[[col for col in phrase_count if not re.search('|'.join(patterns_to_rm), col) and not col in string.punctuation]]\n",
    "\n",
    "phrase_count['podcast_id'] = podcasts['podcast_id'].tolist()\n",
    "phrase_count.drop_duplicates('podcast_id', inplace=True)\n",
    "phrase_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape to long\n",
    "phrase_count_lng = phrase_count.melt(id_vars='podcast_id', var_name='phrase', value_name='tfidf')\n",
    "phrase_count_lng.sort_values(['podcast_id', 'tfidf'], ascending=[True, False], inplace=True)\n",
    "phrase_count_lng.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the top ones\n",
    "top_n = 10\n",
    "phrase_count_top = phrase_count_lng.groupby('podcast_id').head(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dict\n",
    "podcast_id_to_phrase_count = phrase_count_top.groupby('podcast_id')['phrase'].apply(lambda x: x.tolist()).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add colors\n",
    "def add_colors(n, offset=3):\n",
    "    colors = cl.scales['9']['seq']['Blues']\n",
    "    colors = cl.interp(colors, n+offset)\n",
    "    colors = cl.to_numeric(cl.to_rgb(colors))\n",
    "    colors = ['#%02x%02x%02x' % tuple(int(c) for c in color) for color in colors]\n",
    "    return colors[::-1][:-offset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for podcast_id, phrases in podcast_id_to_phrase_count.items():\n",
    "    podcast_id_to_phrase_count[podcast_id] = list(zip(phrases, add_colors(len(phrases))))\n",
    "\n",
    "len(podcast_id_to_phrase_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "pickle.dump(podcast_id_to_phrase_count, open('data/podcast_id_to_phrase_count.pkl', 'wb'), protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (General DS)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
